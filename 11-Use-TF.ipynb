{"cells":[{"cell_type":"markdown","source":["#Using TensorFlow \n* Install TensorFlow on a single node using the following script. This should work fine on the community edition because the driver and the worker are on the same node. \n* NOTE: You will need to use an init script to install on a multi-node cluster and using this script on a multinode cluster will not work, because TensorFlow will only be installed on the driver. The init script solves this problem by running the install script when the worker launches."],"metadata":{}},{"cell_type":"code","source":["%sh\n\n\ntfBinaryUrl=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\"\n\nset -ex\n\necho \"**** Installing CPU-enabled TensorFlow *****\"\n\npip install ${tfBinaryUrl}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+ echo &#39;**** Installing CPU-enabled TensorFlow *****&#39;\n**** Installing CPU-enabled TensorFlow *****\n+ pip3 install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\nERROR: tensorflow-0.11.0-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%sh\npip install --upgrade pip\npip install tensorflow\npip install tf-nightly"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already up-to-date: pip in /databricks/python3/lib/python3.7/site-packages (19.3.1)\nCollecting tensorflow\n  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\nCollecting tensorboard&lt;2.1.0,&gt;=2.0.0\n  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\nCollecting wrapt&gt;=1.11.1\n  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\nCollecting termcolor&gt;=1.1.0\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\nCollecting grpcio&gt;=1.8.6\n  Downloading https://files.pythonhosted.org/packages/bc/b3/0052e38c640d52b710e235b15821cc3c61d0065bf54e70a44550ef127349/grpcio-1.26.0-cp37-cp37m-manylinux2010_x86_64.whl (2.4MB)\nCollecting absl-py&gt;=0.7.0\n  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\nCollecting google-pasta&gt;=0.1.6\n  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\nCollecting astor&gt;=0.6.0\n  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\nCollecting tensorflow-estimator&lt;2.1.0,&gt;=2.0.0\n  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\nRequirement already satisfied: wheel&gt;=0.26 in /databricks/python3/lib/python3.7/site-packages (from tensorflow) (0.33.1)\nCollecting opt-einsum&gt;=2.3.2\n  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\nCollecting keras-applications&gt;=1.0.8\n  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\nCollecting keras-preprocessing&gt;=1.0.5\n  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /databricks/python3/lib/python3.7/site-packages (from tensorflow) (1.16.2)\nCollecting protobuf&gt;=3.6.1\n  Downloading https://files.pythonhosted.org/packages/4a/14/f5c294f1e36a031f165128c25feba93b3116f15a74398d0b2747ed75744f/protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\nCollecting gast==0.2.2\n  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\nCollecting markdown&gt;=2.6.8\n  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.7/site-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2.21.0)\nCollecting setuptools&gt;=41.0.0\n  Downloading https://files.pythonhosted.org/packages/f9/d3/955738b20d3832dfa3cd3d9b07e29a8162edb480bf988332f5e6e48ca444/setuptools-44.0.0-py2.py3-none-any.whl (583kB)\nCollecting werkzeug&gt;=0.11.15\n  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\nCollecting google-auth&lt;2,&gt;=1.6.3\n  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\nCollecting h5py\n  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow) (2.8)\nCollecting pyasn1-modules&gt;=0.2.1\n  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\nCollecting rsa&lt;4.1,&gt;=3.1.4\n  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\nCollecting cachetools&lt;5.0,&gt;=2.0.0\n  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\nCollecting requests-oauthlib&gt;=0.7.0\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nCollecting pyasn1&lt;0.5.0,&gt;=0.4.6\n  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\nCollecting oauthlib&gt;=3.0.0\n  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\nBuilding wheels for collected packages: wrapt, termcolor, absl-py, opt-einsum, gast\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status &#39;done&#39;\n  Created wheel for wrapt: filename=wrapt-1.11.2-cp37-cp37m-linux_x86_64.whl size=66195 sha256=ea994eaf29117a90cd4ca3e44a75c1be0935c97db4a47a86e54e94424ddebd5c\n  Stored in directory: /root/.cache/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n  Building wheel for termcolor (setup.py): started\n  Building wheel for termcolor (setup.py): finished with status &#39;done&#39;\n  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4833 sha256=fcaf57f620632344237b260066408969694dcb830e010c8c2f27dc3a83264d1f\n  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n  Building wheel for absl-py (setup.py): started\n  Building wheel for absl-py (setup.py): finished with status &#39;done&#39;\n  Created wheel for absl-py: filename=absl_py-0.9.0-cp37-none-any.whl size=121932 sha256=4ebc153d96afba24759279130253209fd83d92b20a53542778aacf96b1eaf53d\n  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n  Building wheel for opt-einsum (setup.py): started\n  Building wheel for opt-einsum (setup.py): finished with status &#39;done&#39;\n  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp37-none-any.whl size=61682 sha256=6f77e4b8f1241ffe1a81519c758fbbdc0300ca9792461e2782d129e55002a226\n  Stored in directory: /root/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n  Building wheel for gast (setup.py): started\n  Building wheel for gast (setup.py): finished with status &#39;done&#39;\n  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=ea441ee74da648781fcbe566b44de492d7e7be19689d11bf6b8f6892ac3b3e17\n  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\nSuccessfully built wrapt termcolor absl-py opt-einsum gast\nInstalling collected packages: setuptools, markdown, grpcio, werkzeug, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, absl-py, tensorboard, wrapt, termcolor, google-pasta, astor, tensorflow-estimator, opt-einsum, h5py, keras-applications, keras-preprocessing, gast, tensorflow\n  Found existing installation: setuptools 40.8.0\n    Not uninstalling setuptools at /usr/lib/python3.7/site-packages, outside environment /databricks/python3\n    Can&#39;t uninstall &#39;setuptools&#39;. No files were found to uninstall.\nSuccessfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 gast-0.2.2 google-auth-1.10.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.26.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 oauthlib-3.1.0 opt-einsum-3.1.0 protobuf-3.11.2 pyasn1-0.4.8 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 rsa-4.0 setuptools-44.0.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 werkzeug-0.16.0 wrapt-1.11.2\nCollecting tf-nightly\n  Downloading https://files.pythonhosted.org/packages/9e/06/8f4eec92a92721ddcab892f9fee54bba2e40305ab7d5a52f494cf625bec2/tf_nightly-2.1.0.dev20200104-cp37-cp37m-manylinux2010_x86_64.whl (446.6MB)\nRequirement already satisfied: protobuf&gt;=3.8.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (3.11.2)\nRequirement already satisfied: wrapt&gt;=1.11.1 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.11.2)\nRequirement already satisfied: h5py&lt;2.11.0,&gt;=2.10.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (2.10.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.1.0)\nRequirement already satisfied: grpcio&gt;=1.8.6 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.26.0)\nRequirement already satisfied: absl-py&gt;=0.7.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (0.9.0)\nCollecting scipy==1.4.1; python_version &gt;= &#34;3&#34;\n  Downloading https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1MB)\nRequirement already satisfied: astor&gt;=0.6.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (0.8.1)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (3.1.0)\nCollecting tf-estimator-nightly\n  Downloading https://files.pythonhosted.org/packages/ed/1b/4434a1097e7c717063369121583ed49ff5314a323e77673de78f97a5c462/tf_estimator_nightly-2.0.0.dev2020010409-py2.py3-none-any.whl (453kB)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.16.2)\nRequirement already satisfied: keras-preprocessing&gt;=1.1.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.1.0)\nCollecting tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0\n  Downloading https://files.pythonhosted.org/packages/04/68/146a064e8d02191346d2829692900b267264e3574bf2cd00aee61a732438/tb_nightly-2.2.0a20200104-py3-none-any.whl (3.9MB)\nRequirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (0.33.1)\nRequirement already satisfied: gast==0.2.2 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (0.2.2)\nRequirement already satisfied: google-pasta&gt;=0.1.8 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (0.1.8)\nRequirement already satisfied: six&gt;=1.12.0 in /databricks/python3/lib/python3.7/site-packages (from tf-nightly) (1.12.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.7/site-packages (from protobuf&gt;=3.8.0-&gt;tf-nightly) (44.0.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in /databricks/python3/lib/python3.7/site-packages (from tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (3.1.1)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /databricks/python3/lib/python3.7/site-packages (from tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (2.21.0)\nRequirement already satisfied: werkzeug&gt;=0.11.15 in /databricks/python3/lib/python3.7/site-packages (from tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (0.16.0)\nRequirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /databricks/python3/lib/python3.7/site-packages (from tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (1.10.0)\nRequirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /databricks/python3/lib/python3.7/site-packages (from tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (0.4.1)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (2.8)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /databricks/python3/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (0.2.7)\nRequirement already satisfied: rsa&lt;4.1,&gt;=3.1.4 in /databricks/python3/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (4.0)\nRequirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /databricks/python3/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (4.0.0)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /databricks/python3/lib/python3.7/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (1.3.0)\nRequirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /databricks/python3/lib/python3.7/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (0.4.8)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /databricks/python3/lib/python3.7/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tb-nightly&lt;2.3.0a0,&gt;=2.2.0a0-&gt;tf-nightly) (3.1.0)\nInstalling collected packages: scipy, tf-estimator-nightly, tb-nightly, tf-nightly\n  Found existing installation: scipy 1.2.1\n    Uninstalling scipy-1.2.1:\n      Successfully uninstalled scipy-1.2.1\nSuccessfully installed scipy-1.4.1 tb-nightly-2.2.0a20200104 tf-estimator-nightly-2.0.0.dev2020010409 tf-nightly-2.1.0.dev20200104\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["With python, it is normally only necessary to install a module before you can start using it.  However, when working in a notebook, python is already “running”.  So, python will not notice that the TensorFlow module is available unless you “restart” python.  \n\nThis is simpler than it sounds.  Simply use the cluster menu to `Detach` from your cluster.  Then use the menu again to `Attach` to the cluster.  The notebook will be executed again with a fresh instance of python, and you should be able to import TensorFlow as you normally would.\n\nTry importing `tensorflow` now as `tf`, and `print` `__version__`."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.__version__"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: &#39;2.1.0-dev20200104&#39;</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Some of this code is licensed by Google under the Apache 2.0 License\n\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Load data"],"metadata":{}},{"cell_type":"code","source":["mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Define the model"],"metadata":{}},{"cell_type":"code","source":["x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.matmul(x, W) + b"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Define loss and optimizer"],"metadata":{}},{"cell_type":"code","source":["y_ = tf.placeholder(tf.float32, [None, 10])"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Train our model using small batches of data."],"metadata":{}},{"cell_type":"code","source":["sess = tf.InteractiveSession()\n\ntf.initialize_all_variables().run()\nfor _ in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Test the trained model. The final accuracy is reported at the bottom. You can compare it with the accuracy reported by the other frameworks!"],"metadata":{}},{"cell_type":"code","source":["correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                    y_: mnist.test.labels}))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Distributed processing of images using TensorFlow"],"metadata":{}},{"cell_type":"code","source":["# Settings for this notebook\n\nMODEL_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\nmodel_dir = '/tmp/imagenet'\n\nIMAGES_INDEX_URL = 'http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz'\nimages_read_limit = 1000L  # Increase this to read more images\n\n# Number of images per batch.\n# 1 batch corresponds to 1 RDD row.\nimage_batch_size = 3\n\nnum_top_predictions = 5"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["import numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.platform import gfile\nimport os.path\nimport re\nimport sys\nimport tarfile\nfrom subprocess import Popen, PIPE, STDOUT"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Download the model\nWe download a pre-trained model or find a pre-downloaded one."],"metadata":{}},{"cell_type":"code","source":["def maybe_download_and_extract():\n  \"\"\"Download and extract model tar file.\"\"\"\n  from six.moves import urllib\n  dest_directory = model_dir\n  if not os.path.exists(dest_directory):\n    os.makedirs(dest_directory)\n  filename = MODEL_URL.split('/')[-1]\n  filepath = os.path.join(dest_directory, filename)\n  if not os.path.exists(filepath):\n    filepath2, _ = urllib.request.urlretrieve(MODEL_URL, filepath)\n    print(\"filepath2\", filepath2)\n    statinfo = os.stat(filepath)\n    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n  else:\n    print('Data already downloaded:', filepath, os.stat(filepath))\n\nmaybe_download_and_extract()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Load model data\nLoad the model data, and broadcast it for use on Spark workers."],"metadata":{}},{"cell_type":"code","source":["model_path = os.path.join(model_dir, 'classify_image_graph_def.pb')\nwith gfile.FastGFile(model_path, 'rb') as f:\n  model_data = f.read()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["model_data_bc = sc.broadcast(model_data)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Node lookups\nConcepts (as represented by synsets, or groups of synomymous terms) have integer node IDs. This code loads a mapping from node IDs to human-readable strings for each synset."],"metadata":{}},{"cell_type":"code","source":["class NodeLookup(object):\n  \"\"\"Converts integer node IDs to human readable labels.\"\"\"\n\n  def __init__(self,\n               label_lookup_path=None,\n               uid_lookup_path=None):\n    if not label_lookup_path:\n      label_lookup_path = os.path.join(\n          model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')\n    if not uid_lookup_path:\n      uid_lookup_path = os.path.join(\n          model_dir, 'imagenet_synset_to_human_label_map.txt')\n    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n\n  def load(self, label_lookup_path, uid_lookup_path):\n    \"\"\"Loads a human readable English name for each softmax node.\n\n    Args:\n      label_lookup_path: string UID to integer node ID.\n      uid_lookup_path: string UID to human-readable string.\n\n    Returns:\n      dict from integer node ID to human-readable string.\n    \"\"\"\n    if not gfile.Exists(uid_lookup_path):\n      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n    if not gfile.Exists(label_lookup_path):\n      tf.logging.fatal('File does not exist %s', label_lookup_path)\n\n    # Loads mapping from string UID to human-readable string\n    proto_as_ascii_lines = gfile.GFile(uid_lookup_path).readlines()\n    uid_to_human = {}\n    p = re.compile(r'[n\\d]*[ \\S,]*')\n    for line in proto_as_ascii_lines:\n      parsed_items = p.findall(line)\n      uid = parsed_items[0]\n      human_string = parsed_items[2]\n      uid_to_human[uid] = human_string\n\n    # Loads mapping from string UID to integer node ID.\n    node_id_to_uid = {}\n    proto_as_ascii = gfile.GFile(label_lookup_path).readlines()\n    for line in proto_as_ascii:\n      if line.startswith('  target_class:'):\n        target_class = int(line.split(': ')[1])\n      if line.startswith('  target_class_string:'):\n        target_class_string = line.split(': ')[1]\n        node_id_to_uid[target_class] = target_class_string[1:-2]\n\n    # Loads the final mapping of integer node ID to human-readable string\n    node_id_to_name = {}\n    for key, val in node_id_to_uid.items():\n      if val not in uid_to_human:\n        tf.logging.fatal('Failed to locate: %s', val)\n      name = uid_to_human[val]\n      node_id_to_name[key] = name\n\n    return node_id_to_name\n\n  def id_to_string(self, node_id):\n    if node_id not in self.node_lookup:\n      return ''\n    return self.node_lookup[node_id]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["node_lookup = NodeLookup().node_lookup\n# Broadcast node lookup table to use on Spark workers\nnode_lookup_bc = sc.broadcast(node_lookup)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Read index of image files\nWe load an index of image file URLs. We will parallelize this index. Spark workers will process batches of URLs in parallel by downloading the images and running TensorFlow inference on the images."],"metadata":{}},{"cell_type":"code","source":["# Helper methods for reading images\n\ndef run(cmd):\n  p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n  return p.stdout.read()\n\ndef read_file_index():\n  from six.moves import urllib\n  content = urllib.request.urlopen(IMAGES_INDEX_URL)\n  data = content.read(images_read_limit)\n  tmpfile = \"/tmp/imagenet.tgz\"\n  with open(tmpfile, 'wb') as f:\n    f.write(data)\n  run(\"tar -xOzf %s > /tmp/imagenet.txt\" % tmpfile)\n  with open(\"/tmp/imagenet.txt\", 'r') as f:\n    lines = [l.split() for l in f]\n    input_data = [tuple(elts) for elts in lines if len(elts) == 2]\n    return [input_data[i:i+image_batch_size] for i in range(0,len(input_data), image_batch_size)]"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["batched_data = read_file_index()\nprint (\"There are %d batches\" % len(batched_data))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Distributed image processing: TensorFlow on Spark\nThis section contains the main processing code. We first define methods which will be run as tasks on Spark workers. We then use Spark to parallelize the execution of these methods on the image URL dataset."],"metadata":{}},{"cell_type":"code","source":["def run_inference_on_image(sess, img_id, img_url, node_lookup):\n  \"\"\"Download an image, and run inference on it.\n\n  Args:\n    image: Image file URL\n\n  Returns:\n    (image ID, image URL, scores),\n    where scores is a list of (human-readable node names, score) pairs\n  \"\"\"\n  from six.moves import urllib\n  from urllib2 import HTTPError\n  try:\n    image_data = urllib.request.urlopen(img_url, timeout=1.0).read()\n  except:\n    return (img_id, img_url, None)\n  # Some useful tensors:\n  # 'softmax:0': A tensor containing the normalized prediction across\n  #   1000 labels.\n  # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n  #   float description of the image.\n  # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n  #   encoding of the image.\n  # Runs the softmax tensor by feeding the image_data as input to the graph.\n  softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n  try:\n    predictions = sess.run(softmax_tensor,\n                           {'DecodeJpeg/contents:0': image_data})\n  except:\n    # Handle problems with malformed JPEG files\n    return (img_id, img_url, None)\n  predictions = np.squeeze(predictions)\n  top_k = predictions.argsort()[-num_top_predictions:][::-1]\n  scores = []\n  for node_id in top_k:\n    if node_id not in node_lookup:\n      human_string = ''\n    else:\n      human_string = node_lookup[node_id]\n    score = predictions[node_id]\n    scores.append((human_string, score))\n  return (img_id, img_url, scores)\n\ndef apply_inference_on_batch(batch):\n  \"\"\"Apply inference to a batch of images.\n  \n  We do not explicitly tell TensorFlow to use a GPU.\n  It is able to choose between CPU and GPU based on its guess of which will be faster.\n  \"\"\"\n  with tf.Graph().as_default() as g:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(model_data_bc.value)\n    tf.import_graph_def(graph_def, name='')\n    with tf.Session() as sess:\n      labeled = [run_inference_on_image(sess, img_id, img_url, node_lookup_bc.value) for (img_id, img_url) in batch]\n      return [tup for tup in labeled if tup[2] is not None]"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Run TensorFlow on Spark! Actually, this will not run a Spark job yet since it does not involve an RDD action."],"metadata":{}},{"cell_type":"code","source":["urls = sc.parallelize(batched_data)\nlabeled_images = urls.flatMap(apply_inference_on_batch)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Examine results\nWhen we call collect(), we will finally run the Spark job to process our images."],"metadata":{}},{"cell_type":"code","source":["# The name of the cluster on which to install TensorFlow:\nclusterName = \"tensorflow-cpu\"\n\n# TensorFlow binary URL\ntfBinaryUrl = \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\"\n\n# Create the script tempalte, then render it using format.\nscript = \"\"\"#!/usr/bin/env bash\n\nset -ex\n\necho \"**** Installing GPU-enabled TensorFlow *****\"\n\npip install {tfBinaryUrl}\n\"\"\".format(tfBinaryUrl = tfBinaryUrl)\n\n# Write the script to the global environment\ndbutils.fs.mkdirs(\"dbfs:/databricks/init/\")\ndbutils.fs.put(\"dbfs:/databricks/init/%s/install-tensorflow-gpu.sh\" % clusterName, script, True)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Now that the template exists we can create a cluster called `tensorflow-cpu`, since that is the name of the folder where we put the script.\nOnce the `tensorflow-cpu` cluster launches, attach this notebook to it and see if tensorflow is available by trying to import it."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nprint (tf.__version__)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Now you should have a working multi-node Spark cluster.  This means that not only will python work, but Spark will work too.  This step is not necessary in the community edition because all community edition clusters are single-node, with the driver and the worker on the same node.\n\nA single-node professional cluster only includes the driver, there is no way to install the worker on the same node using a professional account."],"metadata":{}},{"cell_type":"code","source":["local_labeled_images = labeled_images.collect()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["local_labeled_images"],"metadata":{},"outputs":[],"execution_count":42}],"metadata":{"name":"11-Use-TF","notebookId":200060826016270},"nbformat":4,"nbformat_minor":0}
